{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bf052e",
   "metadata": {},
   "source": [
    "# A first look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxipred.backend.data_processing import TaxiData\n",
    "from taxipred.utils.constants import ORIGINAL_CSV_PATH,ALTERED_CSV_PATH\n",
    "taxidata = TaxiData(ORIGINAL_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use info to see the column names aswell as number of nullvalues aswell as typing\n",
    "taxidata.df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking valuecounts to see if any categorical columns are misspelled\n",
    "from taxipred.backend.data_processing import find_categorical_columns\n",
    "cat_cols = find_categorical_columns(taxidata.df)\n",
    "for name in cat_cols:\n",
    "    print(taxidata.df[name].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1b077",
   "metadata": {},
   "source": [
    "### no misspelled rows for categorical\n",
    "i found that the actual values inside the categorical columns are misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the actual look of the dataset. to better understand the columns\n",
    "taxidata.df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# figure out important correlations\n",
    "# suspecting abnormally close to 1 correlation in a couple of these columns\n",
    "matrix = taxidata.df.select_dtypes(include=np.number).corr()\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f862af9c",
   "metadata": {},
   "source": [
    "## My Feature Selection Plan and Justification\n",
    "\n",
    "### The Main Predictor\n",
    "After looking at the data, it's obvious that **`Trip_Distance_km`** is the biggest factor for the **`Trip_Price`**. My correlation check proved this with a strong positive value, so it's the main feature I'll be using.\n",
    "\n",
    "***\n",
    "### Dropping Columns to Avoid Data Leakage\n",
    "\n",
    "I'm dropping several columns to ensure my model is realistic and doesn't \"cheat\" by looking at parts of the answer.\n",
    "\n",
    "**Fare Component Columns (`Base_Fare`, Rates, etc.)**\n",
    "\n",
    "My initial thought was that **`Base_Fare`**, **`Per_Km_Rate`**, and **`Per_Minute_Rate`** are used to calculate the final price. The low correlation values were confusing, so I decided to manually verify this to be sure.\n",
    "\n",
    "First, I needed a complete row of data to work with, so I chose **Row 0** since it had no missing values. Based on the column names, I pieced together the most likely formula:\n",
    "\n",
    "`Total Price = Base_Fare + (Trip_Distance_km * Per_Km_Rate) + (Trip_Duration_Minutes * Per_Minute_Rate)`\n",
    "\n",
    "I then plugged in the numbers from Row 0 to test this theory:\n",
    "\n",
    "* **Base Fare:** `3.56`\n",
    "* **Distance Cost:** `19.35 km * 0.80` = `15.48`\n",
    "* **Duration Cost:** `53.82 min * 0.32` = `17.2224`\n",
    "\n",
    "When I summed these components, the result was **36.2624**, which was a perfect match for the actual **`Trip_Price`**. This test confirmed that the price is a direct result of these columns, proving the data leakage I suspected.\n",
    "\n",
    "**The Trip Duration Problem**\n",
    "\n",
    "I'm also dropping **`Trip_Duration_Minutes`**. This was a tricky one since duration and price are clearly connected. However, the column in this dataset is the *actual* time the trip took, which is something I'd only know *after* it's over. For my model to be realistic, it has to predict the price from stuff I'd know at the start.\n",
    "\n",
    "If I had start and stop locations, I would have used an API to get an *estimated* duration and used that as a feature. Since I don't have that, using the actual duration is just cheating.\n",
    "\n",
    "***\n",
    "### Final Approach\n",
    "\n",
    "Based on this, I'll move forward using **`Trip_Distance_km`** and my categorical features: **`Time_of_Day`** , **`Day_of_Week`**,**`Passenger_Count`**,`Traffic_Conditions` to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40b0dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ade646a9",
   "metadata": {},
   "source": [
    "### Repairing Key Columns Using the Fare Formula\n",
    "\n",
    "Now that the exact mathematical formula connecting the fare components has been identified, I can use it as a powerful tool for data repair.\n",
    "\n",
    "By algebraically rearranging this formula, it's possible to calculate and fill in missing values for my key columnsâ€”the target variable **`Trip_Price`** and the main feature **`Trip_Distance_km`**. This is a deterministic process that allows me to repair these values with 100% accuracy, salvaging valuable rows that would otherwise be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba11c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxidata.repair_data_using_algebra()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e56358",
   "metadata": {},
   "source": [
    "### Uing imputation\n",
    "by using imputation i can fill in remaining nulls so long as there isnt more than 1 null value per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over each column as target using the rest as features until it cannot fill anymore nulls\n",
    "# this was the most timeconsuming portion of my project\n",
    "taxidata.repair_using_imputation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97feba",
   "metadata": {},
   "source": [
    "### Drop useless columns\n",
    "now i drop the columns which wont be included in model for predicting trip prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5037d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"Base_Fare\",\n",
    "    \"Per_Km_Rate\",\n",
    "    \"Per_Minute_Rate\",\n",
    "    \"Trip_Duration_Minutes\",\n",
    "]\n",
    "taxidata.drop_columns(columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d688a27",
   "metadata": {},
   "source": [
    "### Remaining nulls\n",
    "checking to see how distribution of remaining nulls look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxidata.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxidata.df.isnull().sum(axis=1).value_counts(normalize=True)*100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0355e7e1",
   "metadata": {},
   "source": [
    "### Dropping 2.3% of the dataset\n",
    "ive decided that dropping the 2.3% of the dataset that contains 2 null values per row is an acceptable loss.\n",
    "reson being the dropped columns have to much importance in the data to retain the rows with the values missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d82ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxidata.df = taxidata.df.dropna()\n",
    "taxidata.df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a8bf0",
   "metadata": {},
   "source": [
    "### Exporting to csv\n",
    "exporting the new dataset to csv for ingesting into the real price predicting ml model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bebddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxidata.to_csv(ALTERED_CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48dc8b",
   "metadata": {},
   "source": [
    "### Testing new dataset\n",
    "just simply loading it in for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(ALTERED_CSV_PATH)\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxipred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
